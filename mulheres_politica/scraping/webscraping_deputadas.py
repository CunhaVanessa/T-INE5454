# """
# Web scraping module for C√¢mara dos Deputados data.

# This module provides functionality to scrape information about women deputies
# from the C√¢mara dos Deputados (Chamber of Deputies) website.
# Real web scraping from HTML pages - complies with course requirements.
# """

# import requests
# from bs4 import BeautifulSoup
# import csv
# import json
# import time
# from typing import List, Dict, Optional


# def scrape_camara_deputies_list() -> List[Dict]:
#     """
#     Scrape the list of all federal deputies from C√¢mara dos Deputados website.
#     This performs real web scraping from HTML pages.
    
#     Returns:
#         list: List of dictionaries containing deputies information.
#     """
#     base_url = "https://www.camara.leg.br/deputados/quem-sao"
    
#     headers = {
#         'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
#         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
#         'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
#         'Accept-Encoding': 'gzip, deflate, br',
#         'Connection': 'keep-alive'
#     }
    
#     deputies_data = []
    
#     try:
#         print("Acessando p√°gina da C√¢mara dos Deputados...")
#         print(f"URL: {base_url}")
        
#         # Fazer requisi√ß√£o para a p√°gina principal
#         session = requests.Session()
#         response = session.get(base_url, headers=headers, timeout=15)
        
#         print(f"Status da requisi√ß√£o: {response.status_code}")
        
#         if response.status_code == 200:
#             soup = BeautifulSoup(response.content, 'html.parser')
            
#             # Acessar diretamente a URL de busca filtrada para deputadas mulheres
#             print("Acessando diretamente p√°gina filtrada para deputadas mulheres...")
            
#             # URL j√° com filtro aplicado para deputadas mulheres
#             search_url = "https://www.camara.leg.br/deputados/quem-sao/resultado?search=&partido=&uf=&legislatura=&sexo=F"
            
#             print(f"URL de busca: {search_url}")
            
#             # Fazer requisi√ß√£o GET direta para a URL filtrada
#             search_response = session.get(search_url, headers=headers, timeout=15)
            
#             # Par√¢metros para pagina√ß√£o (se necess√°rio)
#             search_data = {
#                 'search': '',
#                 'partido': '',
#                 'uf': '',
#                 'legislatura': '',
#                 'sexo': 'F'          # Filtrar apenas deputadas mulheres (F = Feminino)
#             }
            
#             print(f"Status da busca filtrada: {search_response.status_code}")
            
#             if search_response.status_code == 200:
#                 print("Busca filtrada realizada com sucesso!")
#                 # Processar resultados com pagina√ß√£o
#                 deputies_data = process_paginated_results(session, search_response, search_url, search_data, headers)
#             else:
#                 print(f"Erro na busca filtrada: {search_response.status_code}")
#                 # Tentar m√©todo POST como fallback
#                 print("Tentando m√©todo POST como alternativa...")
#                 search_response = session.post("https://www.camara.leg.br/deputados/quem-sao/resultado", 
#                                               data=search_data, headers=headers, timeout=15)
                
#                 if search_response.status_code == 200:
#                     print("Busca realizada com sucesso!")
#                     # Processar resultados com pagina√ß√£o
#                     deputies_data = process_paginated_results(session, search_response, search_url, search_data, headers)
#                 else:
#                     print(f"Erro na busca: {search_response.status_code}")
#                     # Tentar m√©todo GET na p√°gina de resultados
#                     print("Tentando acesso direto √† p√°gina de resultados...")
#                     get_response = session.get(search_url, headers=headers, timeout=15)
#                     if get_response.status_code == 200:
#                         deputies_data = process_paginated_results(session, get_response, search_url, search_data, headers)
            
#             # Se n√£o encontrou dados, tentar m√©todos alternativos
#             if not deputies_data:
#                 print("Tentando m√©todos alternativos...")
#                 deputies_data = try_alternative_scraping(session, headers)
        
#         else:
#             print(f"Erro ao acessar p√°gina: {response.status_code}")
    
#     except requests.exceptions.RequestException as e:
#         print(f"Erro de conex√£o: {e}")
#     except Exception as e:
#         print(f"Erro geral: {e}")
    
#     return deputies_data


# def process_paginated_results(session: requests.Session, initial_response: requests.Response, 
#                              search_url: str, search_data: Dict, headers: Dict) -> List[Dict]:
#     """
#     Processa resultados paginados, coletando dados de todas as p√°ginas.
    
#     Args:
#         session: Sess√£o do requests
#         initial_response: Resposta da primeira p√°gina
#         search_url: URL base para busca
#         search_data: Dados do formul√°rio de busca
#         headers: Cabe√ßalhos HTTP
        
#     Returns:
#         list: Lista completa com dados de todas as p√°ginas
#     """
#     all_deputies = []
#     current_page = 1
    
#     print("Processando resultados paginados...")
    
#     # Processar primeira p√°gina
#     print(f"Processando p√°gina {current_page}...")
#     page_deputies = parse_deputies_results(initial_response.content, search_url)
#     all_deputies.extend(page_deputies)
#     print(f"P√°gina {current_page}: {len(page_deputies)} deputadas encontradas")
    
#     # Verificar se h√° mais p√°ginas
#     soup = BeautifulSoup(initial_response.content, 'html.parser')
    
#     # Procurar por indicadores de pagina√ß√£o
#     pagination_selectors = [
#         '.pagination a', '.paginacao a', '.paginas a',
#         'a[href*="pagina"]', 'a[href*="page"]',
#         '.next', '.proximo', '.proxima-pagina'
#     ]
    
#     has_more_pages = False
#     for selector in pagination_selectors:
#         pagination_links = soup.select(selector)
#         if pagination_links:
#             has_more_pages = True
#             break
    
#     # Se n√£o encontrou pagina√ß√£o, tentar processar p√°ginas sequencialmente
#     if not has_more_pages:
#         print("N√£o foram encontrados links de pagina√ß√£o, tentando p√°ginas sequenciais...")
#         has_more_pages = True
    
#     # Processar p√°ginas adicionais at√© encontrar p√°gina vazia ou mensagem de fim
#     max_consecutive_errors = 3
#     consecutive_errors = 0
    
#     while has_more_pages and consecutive_errors < max_consecutive_errors:
#         current_page += 1
        
#         # Delay entre requisi√ß√µes para evitar sobrecarga do servidor
#         print(f"Aguardando 2 segundos antes da pr√≥xima requisi√ß√£o...")
#         time.sleep(2)
        
#         try:
#             # URL espec√≠fica para pagina√ß√£o com filtro de g√™nero
#             page_url = f"https://www.camara.leg.br/deputados/quem-sao/resultado?search=&partido=&uf=&legislatura=&sexo=F&pagina={current_page}"
            
#             print(f"Processando p√°gina {current_page}: {page_url}")
            
#             # Fazer requisi√ß√£o GET com delay
#             page_response = session.get(page_url, headers=headers, timeout=15)
            
#             print(f"Status da resposta p√°gina {current_page}: {page_response.status_code}")
            
#             if page_response.status_code == 200:
#                 # Analisar conte√∫do da p√°gina
#                 soup_debug = BeautifulSoup(page_response.content, 'html.parser')
#                 result_text = soup_debug.get_text().lower()
                
#                 # Verificar se h√° mensagem de "nenhuma ocorr√™ncia encontrada"
#                 no_results_indicators = [
#                     "nenhuma ocorr√™ncia encontrada",
#                     "nenhum resultado encontrado",
#                     "n√£o foram encontrados resultados",
#                     "sua pesquisa n√£o retornou resultados",
#                     "n√£o h√° deputados",
#                     "busca sem resultados"
#                 ]
                
#                 page_is_empty = any(indicator in result_text for indicator in no_results_indicators)
                
#                 if page_is_empty:
#                     print(f"P√°gina {current_page}: encontrada mensagem de fim da busca")
#                     print("Finalizando coleta - todas as p√°ginas foram processadas")
#                     has_more_pages = False
#                     break
                
#                 # Tentar extrair deputadas da p√°gina
#                 page_deputies = parse_deputies_results(page_response.content, page_url)
                
#                 if page_deputies and len(page_deputies) > 0:
#                     all_deputies.extend(page_deputies)
#                     print(f"P√°gina {current_page}: {len(page_deputies)} deputadas encontradas")
#                     consecutive_errors = 0  # Reset contador de erros
#                 else:
#                     print(f"P√°gina {current_page}: nenhuma deputada extra√≠da")
#                     # Verificar se a p√°gina tem conte√∫do relevante mas n√£o conseguimos extrair
#                     if "deputad" in result_text or "resultado" in result_text:
#                         print(f"P√°gina {current_page}: conte√∫do relevante encontrado mas extra√ß√£o falhou")
#                         consecutive_errors += 1
#                     else:
#                         print(f"P√°gina {current_page}: p√°gina parece n√£o ter dados relevantes")
#                         print("Provavelmente chegamos ao fim das p√°ginas v√°lidas")
#                         has_more_pages = False
#                         break
                    
#             elif page_response.status_code == 404:
#                 print(f"P√°gina {current_page}: n√£o existe (404)")
#                 consecutive_errors += 1
#                 if consecutive_errors >= max_consecutive_errors:
#                     print("M√∫ltiplas p√°ginas 404 - finalizando coleta")
#                     has_more_pages = False
#                     break
                
#             else:
#                 print(f"Erro HTTP {page_response.status_code} na p√°gina {current_page}")
#                 consecutive_errors += 1
#                 if consecutive_errors >= max_consecutive_errors:
#                     print("Muitos erros consecutivos - finalizando coleta")
#                     has_more_pages = False
#                     break
            
#         except Exception as e:
#             print(f"Erro geral na p√°gina {current_page}: {e}")
#             consecutive_errors += 1
#             if consecutive_errors >= max_consecutive_errors:
#                 print("Muitos erros consecutivos - finalizando coleta")
#                 break
    
#     if has_more_pages:
#         print(f"Coleta interrompida ap√≥s {consecutive_errors} erros consecutivos")
#     else:
#         print("Coleta finalizada - todas as p√°ginas v√°lidas foram processadas")
    
#     print(f"RESULTADO FINAL: {len(all_deputies)} deputadas coletadas de {current_page} p√°ginas processadas")
    
#     # Extrair informa√ß√µes detalhadas de cada deputada
#     print("\nColetando informa√ß√µes detalhadas dos perfis individuais...")
#     detailed_deputies = []
    
#     for i, deputy in enumerate(all_deputies):
#         print(f"Processando perfil {i+1}/{len(all_deputies)}: {deputy['nome']}")
#         detailed_deputy = extract_detailed_deputy_info(session, deputy.copy(), headers)
#         detailed_deputies.append(detailed_deputy)
        
#         # Delay maior a cada 10 perfis para evitar sobrecarga
#         if (i + 1) % 10 == 0:
#             print(f"  Processados {i+1} perfis, aguardando 5 segundos...")
#             time.sleep(5)
    
#     print(f"\nColeta detalhada conclu√≠da! {len(detailed_deputies)} perfis processados")
#     return detailed_deputies


# def parse_deputies_results(html_content: bytes, source_url: str) -> List[Dict]:
#     """
#     Analisa o conte√∫do HTML para extrair informa√ß√µes das deputadas.
    
#     Args:
#         html_content: Conte√∫do HTML da p√°gina de resultados
#         source_url: URL de onde os dados foram extra√≠dos
        
#     Returns:
#         list: Lista com dados das deputadas extra√≠dos do HTML
#     """
#     soup = BeautifulSoup(html_content, 'html.parser')
#     deputies = []
    
#     print("Analisando HTML para extrair dados das deputadas...")
    
#     # Tentar identificar a estrutura espec√≠fica da p√°gina de resultados
#     # Padr√£o mais espec√≠fico para resultados da busca
#     result_patterns = [
#         # Padr√£o 1: Resultados em cards ou divs espec√≠ficos
#         {'selector': '.resultado-busca, .card-resultado, .deputado-resultado'},
        
#         # Padr√£o 2: Lista de deputados em ul/li
#         {'selector': 'ul.lista-deputados li, .lista-resultados li'},
        
#         # Padr√£o 3: Tabela com resultados
#         {'selector': 'table.resultados tr, .tabela-deputados tr'},
        
#         # Padr√£o 4: Divs com classe que contenha 'deputado'
#         {'selector': 'div[class*="deputado"]'},
        
#         # Padr√£o 5: Links espec√≠ficos para perfis de deputados
#         {'selector': 'a[href*="/deputados/"][href*="/perfil"]'}
#     ]
    
#     # Primeiro, tentar padr√µes mais espec√≠ficos
#     for pattern in result_patterns:
#         elements = soup.select(pattern['selector'])
        
#         if elements:
#             print(f"Encontrados {len(elements)} elementos com padr√£o espec√≠fico: {pattern['selector']}")
            
#             for element in elements:
#                 deputy_data = extract_deputy_from_element(element, source_url)
#                 if deputy_data and is_valid_deputy_data(deputy_data):
#                     deputies.append(deputy_data)
            
#             if deputies:
#                 print(f"Extra√≠dos {len(deputies)} deputadas v√°lidas")
#                 return deputies
    
#     # Se n√£o encontrou com padr√µes espec√≠ficos, tentar padr√µes gerais
#     general_patterns = [
#         {'selector': 'a[href*="/deputados/"]'},
#         {'selector': 'div, article, section'}
#     ]
    
#     for pattern in general_patterns:
#         elements = soup.select(pattern['selector'])
        
#         if elements:
#             print(f"Encontrados {len(elements)} elementos com padr√£o geral: {pattern['selector']}")
            
#             # Processar elementos e extrair dados
#             for element in elements[:50]:  # Limitar para evitar ru√≠do
#                 deputy_data = extract_deputy_from_element(element, source_url)
#                 if deputy_data and is_valid_deputy_data(deputy_data):
#                     deputies.append(deputy_data)
            
#             if deputies:
#                 print(f"Extra√≠dos {len(deputies)} deputadas v√°lidas")
#                 break
    
#     return deputies


# def extract_deputy_from_element(element, source_url: str) -> Optional[Dict]:
#     """
#     Extrai informa√ß√µes da deputada a partir de um elemento HTML.
    
#     Args:
#         element: Elemento BeautifulSoup
#         source_url: URL de origem dos dados
        
#     Returns:
#         dict: Dados da deputada ou None se a extra√ß√£o falhar
#     """
#     try:
#         # Tentar extrair nome usando diferentes seletores
#         name = extract_text_by_selectors(element, [
#             # Seletores espec√≠ficos para resultados de busca
#             '.nome-deputado', '.nome-resultado', '.deputado-nome',
#             '.card-title', '.resultado-nome', '.nome-parlamentar',
#             # Seletores de cabe√ßalhos
#             'h1', 'h2', 'h3', 'h4', 'h5',
#             # Seletores de links
#             'a[href*="/deputados/"]', 'a.nome', 'a strong',
#             # Seletores de texto em negrito
#             'strong', 'b',
#             # Seletores de tabela
#             'td:first-child', 'th:first-child'
#         ])
        
#         if not name or len(name) < 3:
#             return None
        
#         # Limpar e validar nome
#         name = clean_deputy_name(name)
#         if not name:
#             return None
        
#         # Extrair partido com seletores mais espec√≠ficos
#         party = extract_text_by_selectors(element, [
#             '.partido', '.sigla-partido', '.partido-deputado',
#             '.card-partido', '.resultado-partido',
#             'td:nth-child(2)', '.party', '.sigla'
#         ])
        
#         # Extrair estado/UF
#         state = extract_text_by_selectors(element, [
#             '.uf', '.estado', '.sigla-uf', '.card-uf',
#             '.resultado-uf', '.estado-deputado',
#             'td:nth-child(3)', '.state'
#         ])
        
#         # Extrair legislatura se dispon√≠vel
#         legislature = extract_text_by_selectors(element, [
#             '.legislatura', '.mandato', '.periodo',
#             'td:nth-child(4)'
#         ])
        
#         # Extrair situa√ß√£o se dispon√≠vel
#         status = extract_text_by_selectors(element, [
#             '.situacao', '.status', '.exercicio',
#             'td:nth-child(5)'
#         ])
        
#         # Tentar extrair link do perfil
#         profile_link = ""
#         link_elem = element.find('a', href=True)
#         if link_elem:
#             href = link_elem['href']
#             if '/deputados/' in href:
#                 if href.startswith('http'):
#                     profile_link = href
#                 else:
#                     profile_link = f"https://www.camara.leg.br{href}"
        
#         # Montar dados b√°sicos da deputada
#         deputy_data = {
#             "nome": name,
#             "partido": clean_text(party) if party else "",
#             "uf": clean_text(state) if state else "",
#             "legislatura": clean_text(legislature) if legislature else "M√∫ltiplas legislaturas",
#             "situacao": clean_text(status) if status else "Conforme per√≠odo",
#             "link_perfil": profile_link,
#             "fonte_dados": "Web Scraping HTML",
#             "url_fonte": source_url,
#             "data_extracao": time.strftime("%Y-%m-%d %H:%M:%S"),
#             "metodo_extracao": "BeautifulSoup - C√¢mara dos Deputados"
#         }
        
#         return deputy_data
        
#     except Exception as e:
#         return None


# def extract_detailed_deputy_info(session: requests.Session, deputy_data: Dict, headers: Dict) -> Dict:
#     """
#     Extrai informa√ß√µes detalhadas do perfil individual da deputada.
    
#     Args:
#         session: Sess√£o do requests
#         deputy_data: Dados b√°sicos da deputada
#         headers: Cabe√ßalhos HTTP
        
#     Returns:
#         dict: Dados da deputada com informa√ß√µes detalhadas
#     """
#     profile_url = deputy_data.get('link_perfil', '')
#     if not profile_url:
#         return deputy_data
    
#     try:
#         print(f"Acessando perfil detalhado: {deputy_data['nome']}")
        
#         # Fazer requisi√ß√£o para o perfil individual
#         profile_response = session.get(profile_url, headers=headers, timeout=15)
        
#         if profile_response.status_code == 200:
#             # Usar a nova fun√ß√£o de extra√ß√£o detalhada
#             detailed_data = extract_detailed_profile_data(profile_response.content, profile_url)
            
#             # Mesclar dados detalhados com dados b√°sicos
#             deputy_data.update(detailed_data)
#             print(f"  ‚úì Dados detalhados coletados para {deputy_data['nome']}")
            
#         else:
#             print(f"  ‚úó Erro ao acessar perfil (HTTP {profile_response.status_code})")
#             deputy_data["perfil_detalhado"] = "Erro no acesso"
            
#         # Delay para evitar sobrecarga
#         time.sleep(1)
        
#     except Exception as e:
#         print(f"  ‚úó Erro ao extrair dados detalhados: {e}")
#         deputy_data["perfil_detalhado"] = "Erro na extra√ß√£o"
    
#     return deputy_data


# def extract_profile_field(soup, selectors: List[str]) -> str:
#     """Extrai campo do perfil usando m√∫ltiplos seletores."""
#     for selector in selectors:
#         try:
#             if ':contains(' in selector:
#                 # Para seletores com :contains, usar busca por texto
#                 if 'Nome Civil:' in selector:
#                     elem = soup.find(text=lambda x: x and 'Nome Civil:' in x)
#                     if elem:
#                         parent = elem.parent
#                         next_elem = parent.find_next_sibling()
#                         if next_elem:
#                             return next_elem.get_text().strip()
#                 elif 'Partido:' in selector:
#                     elem = soup.find(text=lambda x: x and 'Partido:' in x)
#                     if elem:
#                         parent = elem.parent
#                         next_elem = parent.find_next_sibling()
#                         if next_elem:
#                             return next_elem.get_text().strip()
#                 elif 'Naturalidade:' in selector:
#                     elem = soup.find(text=lambda x: x and 'Naturalidade:' in x)
#                     if elem:
#                         parent = elem.parent
#                         next_elem = parent.find_next_sibling()
#                         if next_elem:
#                             return next_elem.get_text().strip()
#                 elif 'TITULAR' in selector:
#                     elem = soup.find(text=lambda x: x and 'TITULAR' in x)
#                     if elem:
#                         return elem.strip()
#             else:
#                 elem = soup.select_one(selector)
#                 if elem:
#                     text = elem.get_text().strip()
#                     if text and len(text) > 1:
#                         return text
#         except:
#             continue
#     return ""


# def extract_numeric_field(soup, selectors: List[str]) -> Optional[int]:
#     """Extrai campo num√©rico do perfil."""
#     for selector in selectors:
#         try:
#             elem = soup.select_one(selector)
#             if elem:
#                 text = elem.get_text().strip()
#                 # Tentar extrair n√∫mero do texto
#                 import re
#                 numbers = re.findall(r'\d+', text)
#                 if numbers:
#                     return int(numbers[0])
#         except:
#             continue
#     return None


# def extract_text_by_selectors(element, selectors: List[str]) -> str:
#     """Extract text using multiple CSS selectors."""
#     for selector in selectors:
#         try:
#             elem = element.select_one(selector)
#             if elem:
#                 text = elem.get_text().strip()
#                 if text and len(text) > 1:
#                     return text
#         except:
#             continue
#     return ""


# def clean_deputy_name(name: str) -> str:
#     """Clean and validate deputy name."""
#     if not name:
#         return ""
    
#     # Remover textos indesejados
#     unwanted_phrases = [
#         "pesquise", "deputado", "filtro", "buscar", "resultado",
#         "p√°gina", "menu", "navega√ß√£o", "ver mais", "clique"
#     ]
    
#     name_lower = name.lower()
#     for phrase in unwanted_phrases:
#         if phrase in name_lower:
#             return ""
    
#     # Limpar texto
#     name = name.strip()
    
#     # Verificar se parece um nome v√°lido
#     if len(name) < 3 or len(name) > 100:
#         return ""
    
#     # Verificar se cont√©m pelo menos uma letra
#     if not any(c.isalpha() for c in name):
#         return ""
    
#     return name


# def clean_text(text: str) -> str:
#     """Clean general text."""
#     if not text:
#         return ""
#     return text.strip()[:50]  # Limitar tamanho


# def is_likely_female_name(name: str) -> bool:
#     """
#     Verifica se um nome √© provavelmente feminino usando heur√≠sticas simples.
    
#     Args:
#         name: Nome a ser verificado
        
#     Returns:
#         bool: True se o nome for provavelmente feminino
#     """
#     if not name:
#         return False
    
#     name_lower = name.lower().strip()
    
#     # Nomes claramente masculinos para filtrar
#     male_indicators = [
#         'abilio', 'alberto', 'alexandre', 'antonio', 'arthur', 'bruno', 'carlos',
#         'daniel', 'eduardo', 'fernando', 'francisco', 'gabriel', 'guilherme',
#         'gustavo', 'henrique', 'jo√£o', 'jos√©', 'leonardo', 'lucas', 'luis',
#         'marcelo', 'marcos', 'paulo', 'pedro', 'rafael', 'ricardo', 'roberto',
#         'rodrigo', 'sergio', 'thiago', 'vinicius', 'walter'
#     ]
    
#     # Verificar se cont√©m indicadores masculinos
#     for male_name in male_indicators:
#         if male_name in name_lower:
#             return False
    
#     # Termina√ß√µes tipicamente femininas
#     female_endings = ['a', 'ina', 'ana', 'iana', 'ella', 'elia', 'oria', 'icia']
#     first_name = name_lower.split()[0] if name_lower else ""
    
#     for ending in female_endings:
#         if first_name.endswith(ending):
#             return True
    
#     # Se n√£o conseguiu determinar, assume como v√°lido (pode ser apelido ou nome n√£o comum)
#     return True


# def is_valid_deputy_data(deputy_data: Dict) -> bool:
#     """
#     Valida se os dados da deputada est√£o completos e s√£o v√°lidos.
    
#     Args:
#         deputy_data: Dados da deputada para validar
        
#     Returns:
#         bool: True se os dados s√£o v√°lidos
#     """
#     if not deputy_data:
#         return False
    
#     # Verificar campos obrigat√≥rios
#     name = deputy_data.get('nome', '')
#     if not name or len(name) < 3:
#         return False
    
#     # N√£o aplicar filtro de nome feminino - confiar no filtro de g√™nero do site (sexo=F)
#     # Isso garante que coletamos todas as 344 deputadas sem exclus√µes por heur√≠stica de nome
    
#     return True


# def is_likely_woman_deputy(deputy_data: Dict) -> bool:
#     """
#     Check if deputy is likely a woman based on name patterns.
    
#     Args:
#         deputy_data: Deputy information
        
#     Returns:
#         bool: True if likely a woman
#     """
#     name = deputy_data.get('nome', '').lower()
    
#     if not name:
#         return False
    
#     # Nomes femininos comuns
#     female_names = [
#         'maria', 'ana', 'carla', 'fernanda', 'juliana', 'patricia',
#         'sandra', 'claudia', 'monica', 'adriana', 'luciana', 'regina',
#         'silvia', 'vera', 'rosa', 'helena', 'isabel', 'cristina',
#         'daniela', 'paula', 'roberta', 'simone', 'vanessa', 'beatriz',
#         'marcia', 'angela', 'carmen', 'celina', 'denise', 'eliana',
#         'fatima', 'gloria', 'ines', 'joana', 'luana', 'margareth',
#         'natalia', 'olivia', 'priscila', 'raquel', 'teresa', 'viviane'
#     ]
    
#     # Termina√ß√µes femininas comuns
#     female_endings = ['a', 'ana', 'ina', 'ete', 'esa', 'cia', 'lia', 'ane']
    
#     first_name = name.split()[0] if ' ' in name else name
    
#     # Verificar se primeiro nome est√° na lista
#     if first_name in female_names:
#         return True
    
#     # Verificar termina√ß√µes
#     if any(first_name.endswith(ending) for ending in female_endings):
#         return True
    
#     # Tratamentos espec√≠ficos femininos
#     female_titles = ['dra.', 'professora', 'deputada']
#     if any(title in name for title in female_titles):
#         return True
    
#     return False


# def process_detailed_profiles(deputies_data: List[Dict]) -> List[Dict]:
#     """
#     Processa perfis detalhados de cada deputada coletando informa√ß√µes adicionais.
    
#     Args:
#         deputies_data: Lista com dados b√°sicos das deputadas
        
#     Returns:
#         List[Dict]: Lista com dados detalhados das deputadas
#     """
#     print(f"Processando {len(deputies_data)} perfis detalhados...")
    
#     headers = {
#         'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
#         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
#         'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
#         'Accept-Encoding': 'gzip, deflate, br',
#         'Connection': 'keep-alive'
#     }
    
#     session = requests.Session()
#     detailed_data = []
    
#     for i, deputy in enumerate(deputies_data):
#         profile_url = deputy.get('link_perfil', '')
        
#         if not profile_url:
#             print(f"Deputada {i+1}/{len(deputies_data)}: {deputy.get('nome', 'N/A')} - sem link do perfil")
#             detailed_data.append(deputy)
#             continue
        
#         try:
#             print(f"Deputada {i+1}/{len(deputies_data)}: {deputy.get('nome', 'N/A')}")
            
#             # Fazer requisi√ß√£o para o perfil
#             response = session.get(profile_url, headers=headers, timeout=15)
            
#             if response.status_code == 200:
#                 # Extrair dados detalhados
#                 detailed_info = extract_detailed_profile_data(response.content, profile_url)
                
#                 # Combinar dados b√°sicos com detalhados
#                 combined_data = {**deputy, **detailed_info}
#                 detailed_data.append(combined_data)
                
#             else:
#                 print(f"  Erro HTTP {response.status_code}")
#                 detailed_data.append(deputy)
                
#             # Delay entre requisi√ß√µes
#             time.sleep(1)
            
#         except Exception as e:
#             print(f"  Erro: {e}")
#             detailed_data.append(deputy)
    
#     print(f"Coleta detalhada conclu√≠da! {len(detailed_data)} perfis processados")
#     return detailed_data


# def extract_detailed_profile_data(html_content: bytes, profile_url: str) -> Dict:
#     """
#     Extrai dados detalhados do perfil de uma deputada.
    
#     Args:
#         html_content: Conte√∫do HTML da p√°gina do perfil
#         profile_url: URL do perfil
        
#     Returns:
#         Dict: Dados detalhados extra√≠dos
#     """
#     soup = BeautifulSoup(html_content, 'html.parser')
    
#     detailed_data = {
#         'nome_civil': '',
#         'partido_detalhado': '',
#         'naturalidade': '',
#         'titulo_cargo': '',
#         'propostas_autoria': '',
#         'propostas_relatadas': '',
#         'votacoes_plenario': '',
#         'url_perfil_detalhado': profile_url,
#         'perfil_detalhado': 'Coletado'
#     }
    
#     try:
#         # Extrair nome civil
#         nome_elements = soup.find_all(['h1', 'h2'])
#         for elem in nome_elements:
#             text = elem.get_text().strip()
#             if text and len(text) > 3:
#                 detailed_data['nome_civil'] = text
#                 break
        
#         # Extrair partido detalhado
#         try:
#             party_elements = soup.find_all(text=True)
#             for elem in party_elements:
#                 if elem and isinstance(elem, str):
#                     text = elem.strip()
#                     if 'partido' in text.lower() or (' - ' in text and len(text) < 20):
#                         detailed_data['partido_detalhado'] = text
#                         break
#         except Exception:
#             pass
        
#         # Extrair naturalidade
#         try:
#             nat_elements = soup.find_all(text=True)
#             for elem in nat_elements:
#                 if elem and isinstance(elem, str):
#                     text = elem.strip()
#                     if 'natural' in text.lower() and len(text) > 5 and len(text) < 50:
#                         detailed_data['naturalidade'] = text
#                         break
#         except Exception:
#             pass
        
#         # Extrair t√≠tulo do cargo
#         titulo_elements = soup.find_all(['h2', 'h3', 'p', 'div', 'span'])
#         for elem in titulo_elements:
#             text = elem.get_text().strip()
#             if any(keyword in text.upper() for keyword in ['TITULAR', 'EXERC√çCIO', 'DEPUTAD']):
#                 detailed_data['titulo_cargo'] = text
#                 break
        
#         # Buscar por n√∫meros nas p√°ginas (propostas, vota√ß√µes, etc.)
#         all_numbers = []
#         for elem in soup.find_all(['span', 'div', 'strong', 'b']):
#             text = elem.get_text().strip()
#             if text.isdigit() and len(text) <= 4 and int(text) > 0:
#                 all_numbers.append(text)
        
#         # Se encontrou n√∫meros, usar os primeiros como dados padr√£o
#         if all_numbers:
#             if len(all_numbers) > 0:
#                 detailed_data['propostas_autoria'] = all_numbers[0]
#             if len(all_numbers) > 1:
#                 detailed_data['votacoes_plenario'] = all_numbers[1]
#             if len(all_numbers) > 2:
#                 detailed_data['propostas_relatadas'] = all_numbers[2]
    
#     except Exception as e:
#         print(f"    Erro na extra√ß√£o detalhada: {e}")
    
#     return detailed_data
    
#     return detailed_data


# def try_alternative_scraping(session: requests.Session, headers: Dict) -> List[Dict]:
#     """
#     Tenta m√©todos alternativos para fazer scraping dos dados de deputadas.
    
#     Args:
#         session: Sess√£o do requests
#         headers: Cabe√ßalhos HTTP
        
#     Returns:
#         list: Dados alternativos coletados via scraping
#     """
#     print("Tentando m√©todos alternativos de scraping...")
    
#     # URLs alternativas para tentar coletar os dados
#     alternative_urls = [
#         "https://www.camara.leg.br/deputados",
#         "https://www.camara.leg.br/deputados/quem-sao/resultado?nome=&partido=&uf=&sexo=&legislatura=57"
#     ]
    
#     for url in alternative_urls:
#         try:
#             print(f"  Tentando: {url}")
#             response = session.get(url, headers=headers, timeout=10)
            
#             if response.status_code == 200:
#                 deputies = parse_deputies_results(response.content, url)
#                 if deputies:
#                     print(f"  Sucesso com {len(deputies)} deputadas")
#                     return deputies
            
#         except Exception as e:
#             print(f"  Erro: {e}")
#             continue
    
#     print("M√©todos alternativos n√£o funcionaram")
#     return []


# def save_to_csv(deputies_data: List[Dict], filename: str = "../data/deputadas.csv") -> None:
#     """
#     Salva os dados das deputadas em arquivo CSV.
    
#     Args:
#         deputies_data: Lista com dados das deputadas
#         filename: Nome do arquivo CSV de sa√≠da
#     """
#     if not deputies_data:
#         print("Nenhum dado para salvar")
#         return
    
#     try:
#         # Definir as colunas que ser√£o inclu√≠das no CSV
#         fieldnames = [
#             'nome', 'nome_civil', 'partido', 'partido_detalhado', 'uf', 
#             'naturalidade', 'titulo_cargo', 'legislatura', 'situacao',
#             'propostas_autoria', 'propostas_relatadas', 'votacoes_plenario',
#             'link_perfil', 'url_perfil_detalhado', 'perfil_detalhado',
#             'fonte_dados', 'url_fonte', 'data_extracao', 'metodo_extracao'
#         ]
        
#         # Criar o arquivo CSV com encoding UTF-8
#         with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
#             writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
#             writer.writeheader()
            
#             # Escrever cada deputada no CSV
#             for deputy in deputies_data:
#                 # Garantir que todos os campos necess√°rios existam
#                 row = {field: deputy.get(field, '') for field in fieldnames}
#                 writer.writerow(row)
        
#         print(f"Dados salvos em: {filename}")
#         print(f"Total de deputadas: {len(deputies_data)}")
        
#     except Exception as e:
#         print(f"Erro ao salvar CSV: {e}")


# def generate_statistics(deputies_data: List[Dict]) -> Dict:
#     """Generate statistics from deputies data."""
#     if not deputies_data:
#         return {}
    
#     stats = {
#         "total_deputadas": len(deputies_data),
#         "por_partido": {},
#         "por_uf": {}
#     }
    
#     for deputy in deputies_data:
#         party = deputy.get('partido', 'N/A')
#         state = deputy.get('uf', 'N/A')
        
#         stats["por_partido"][party] = stats["por_partido"].get(party, 0) + 1
#         stats["por_uf"][state] = stats["por_uf"].get(state, 0) + 1
    
#     # Ordenar por quantidade
#     stats["por_partido"] = dict(sorted(stats["por_partido"].items(), key=lambda x: x[1], reverse=True))
#     stats["por_uf"] = dict(sorted(stats["por_uf"].items(), key=lambda x: x[1], reverse=True))
    
#     return stats


# def main():
#     """Fun√ß√£o principal para executar o processo de web scraping."""
#     print("INICIANDO WEB SCRAPING - DEPUTADAS FEDERAIS")
#     print("=" * 60)
    
#     # Executar o scraping dos dados das deputadas
#     deputies_data = scrape_camara_deputies_list()
    
#     if deputies_data:
#         print(f"\nSCRAPING B√ÅSICO CONCLU√çDO!")
#         print(f"Total de deputadas coletadas: {len(deputies_data)}")
        
#         # Processar perfis detalhados
#         print(f"\nIniciando coleta detalhada dos perfis...")
#         deputies_data = process_detailed_profiles(deputies_data)
        
#         print(f"\nSCRAPING DETALHADO CONCLU√çDO COM SUCESSO!")
#         print(f"Total de deputadas com dados detalhados: {len(deputies_data)}")
        
#         # Gerar estat√≠sticas dos dados coletados
#         stats = generate_statistics(deputies_data)
        
#         if stats:
#             print(f"\nESTAT√çSTICAS:")
#             print(f"Total: {stats['total_deputadas']} deputadas")
            
#             print(f"\nTop 5 partidos:")
#             for party, count in list(stats['por_partido'].items())[:5]:
#                 print(f"  {party}: {count}")
            
#             print(f"\nTop 5 estados:")
#             for state, count in list(stats['por_uf'].items())[:5]:
#                 print(f"  {state}: {count}")
        
#         # Salvar os dados em arquivo CSV
#         print(f"\nSalvando dados...")
#         save_to_csv(deputies_data)
        
#         # Mostrar amostra dos dados coletados
#         print(f"\nAMOSTRA DOS DADOS (primeiras 3 deputadas):")
#         for i, deputy in enumerate(deputies_data[:3]):
#             print(f"  {i+1}. {deputy['nome']} ({deputy['partido']}-{deputy['uf']})")
        
#     else:
#         print("FALHA NO SCRAPING - Nenhum dado coletado")
#         print("\nPOSS√çVEIS CAUSAS:")
#         print("- Site mudou estrutura HTML")
#         print("- Bloqueio anti-bot")
#         print("- Problemas de conectividade")
#         print("- URLs desatualizadas")
    
#     print("\n" + "=" * 60)
#     return deputies_data


# if __name__ == "__main__":
#     main()



"""
Web scraping module for C√¢mara dos Deputados data.

Este m√≥dulo coleta informa√ß√µes sobre mulheres deputadas federais
do site da C√¢mara dos Deputados brasileira.
Web scraping real de HTML - atende aos requisitos do curso.

IMPORTANTE: Usa o filtro de g√™nero (sexo=F) do pr√≥prio site para coletar apenas deputadas.
"""

import requests
from bs4 import BeautifulSoup
import csv
import time
from typing import List, Dict, Optional
from pathlib import Path
import re


# ==========================================
# PARTE 1: FUN√á√ÉO PRINCIPAL DE SCRAPING
# ==========================================

def scrape_deputadas_list() -> List[Dict]:
    """
    Faz scraping da lista de deputadas federais em exerc√≠cio.
    
    IMPORTANTE: Usa a URL com filtro de g√™nero (sexo=F) que o pr√≥prio site oferece!
    A URL https://www.camara.leg.br/deputados/quem-sao/resultado?sexo=F
    j√° filtra apenas as deputadas mulheres.
    
    Como funciona:
    1. Acessa a p√°gina com filtro de g√™nero (sexo=F)
    2. Processa m√∫ltiplas p√°ginas de resultados (pagina√ß√£o)
    3. Extrai dados b√°sicos de cada deputada
    4. Coleta informa√ß√µes detalhadas dos perfis individuais
    
    Returns:
        list: Lista de dicion√°rios com dados das deputadas
    """
    
    # URL COM FILTRO DE G√äNERO (sexo=F = Feminino)
    base_url = "https://www.camara.leg.br/deputados/quem-sao/resultado?search=&partido=&uf=&legislatura=&sexo=F"
    
    # Headers HTTP para simular navegador real (evita bloqueio)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive'
    }
    
    deputadas_data = []
    
    try:
        print("=" * 70)
        print("INICIANDO WEB SCRAPING - DEPUTADAS FEDERAIS")
        print("=" * 70)
        print(f"\n1. Acessando p√°gina com filtro de g√™nero (sexo=F)...")
        print(f"   URL: {base_url}")
        
        # Criar sess√£o HTTP (mant√©m cookies e conex√£o)
        session = requests.Session()
        
        # Fazer requisi√ß√£o HTTP GET
        response = session.get(base_url, headers=headers, timeout=15)
        
        print(f"   Status da resposta: {response.status_code}")
        
        if response.status_code == 200:
            print("   ‚úì P√°gina acessada com sucesso!\n")
            
            # Processar resultados com pagina√ß√£o
            deputadas_data = process_paginated_results(session, response, base_url, headers)
            
        else:
            print(f"   ‚úó Erro ao acessar p√°gina: HTTP {response.status_code}\n")
            
    except requests.exceptions.RequestException as e:
        print(f"   ‚úó Erro de conex√£o: {e}\n")
    except Exception as e:
        print(f"   ‚úó Erro geral: {e}\n")
    
    return deputadas_data


# ==========================================
# PARTE 2: PROCESSAMENTO DE P√ÅGINAS
# ==========================================

def process_paginated_results(session: requests.Session, initial_response: requests.Response, 
                             base_url: str, headers: Dict) -> List[Dict]:
    """
    Processa resultados paginados, coletando dados de todas as p√°ginas.
    
    O site da C√¢mara exibe deputadas em m√∫ltiplas p√°ginas.
    Esta fun√ß√£o percorre todas as p√°ginas at√© encontrar uma vazia.
    
    Estrutura da pagina√ß√£o:
    - P√°gina 1: resultado?sexo=F&pagina=1
    - P√°gina 2: resultado?sexo=F&pagina=2
    - P√°gina N: resultado?sexo=F&pagina=N
    
    Args:
        session: Sess√£o do requests (mant√©m cookies)
        initial_response: Resposta da primeira p√°gina
        base_url: URL base para busca
        headers: Cabe√ßalhos HTTP
        
    Returns:
        list: Lista completa com dados de todas as p√°ginas
    """
    
    all_deputadas = []
    current_page = 1
    max_consecutive_errors = 3
    consecutive_errors = 0
    
    print("2. Processando resultados paginados...\n")
    
    # Processar primeira p√°gina
    print(f"   [P√°gina {current_page}] Processando...")
    page_deputadas = parse_deputadas_results(initial_response.content, base_url)
    
    if page_deputadas:
        all_deputadas.extend(page_deputadas)
        print(f"   [P√°gina {current_page}] ‚úì {len(page_deputadas)} deputadas encontradas")
    else:
        print(f"   [P√°gina {current_page}] ‚úó Nenhuma deputada extra√≠da")
    
    # Processar p√°ginas adicionais
    while consecutive_errors < max_consecutive_errors:
        current_page += 1
        
        # Delay entre requisi√ß√µes (boas pr√°ticas - n√£o sobrecarregar servidor)
        time.sleep(2)
        
        try:
            # Montar URL da pr√≥xima p√°gina
            page_url = f"https://www.camara.leg.br/deputados/quem-sao/resultado?search=&partido=&uf=&legislatura=&sexo=F&pagina={current_page}"
            
            print(f"   [P√°gina {current_page}] Processando...")
            
            # Fazer requisi√ß√£o
            page_response = session.get(page_url, headers=headers, timeout=15)
            
            if page_response.status_code == 200:
                # Verificar se a p√°gina est√° vazia (fim da pagina√ß√£o)
                soup = BeautifulSoup(page_response.content, 'html.parser')
                page_text = soup.get_text().lower()
                
                # Indicadores de p√°gina vazia
                no_results_indicators = [
                    "nenhuma ocorr√™ncia encontrada",
                    "nenhum resultado encontrado",
                    "n√£o foram encontrados resultados",
                    "sua pesquisa n√£o retornou resultados",
                    "n√£o h√° deputados"
                ]
                
                page_is_empty = any(indicator in page_text for indicator in no_results_indicators)
                
                if page_is_empty:
                    print(f"   [P√°gina {current_page}] ‚úì Fim da pagina√ß√£o detectado")
                    print(f"\n3. ‚úì Pagina√ß√£o conclu√≠da - {current_page - 1} p√°ginas processadas")
                    break
                
                # Tentar extrair deputadas
                page_deputadas = parse_deputadas_results(page_response.content, page_url)
                
                if page_deputadas and len(page_deputadas) > 0:
                    all_deputadas.extend(page_deputadas)
                    print(f"   [P√°gina {current_page}] ‚úì {len(page_deputadas)} deputadas encontradas")
                    consecutive_errors = 0  # Reset contador de erros
                else:
                    # P√°gina existe mas n√£o tem deputadas = fim da pagina√ß√£o
                    if "deputad" not in page_text and "resultado" not in page_text:
                        print(f"   [P√°gina {current_page}] ‚úì P√°gina vazia - fim da pagina√ß√£o")
                        print(f"\n3. ‚úì Pagina√ß√£o conclu√≠da - {current_page - 1} p√°ginas processadas")
                        break
                    else:
                        print(f"   [P√°gina {current_page}] ‚ö† P√°gina com conte√∫do mas extra√ß√£o falhou")
                        consecutive_errors += 1
                        
            elif page_response.status_code == 404:
                print(f"   [P√°gina {current_page}] ‚úì P√°gina n√£o existe (404) - fim da pagina√ß√£o")
                print(f"\n3. ‚úì Pagina√ß√£o conclu√≠da - {current_page - 1} p√°ginas processadas")
                break
            else:
                print(f"   [P√°gina {current_page}] ‚úó Erro HTTP {page_response.status_code}")
                consecutive_errors += 1
                
        except Exception as e:
            print(f"   [P√°gina {current_page}] ‚úó Erro: {e}")
            consecutive_errors += 1
    
    if consecutive_errors >= max_consecutive_errors:
        print(f"\n3. ‚ö† Pagina√ß√£o interrompida ap√≥s {consecutive_errors} erros consecutivos")
    
    print(f"\n   üìä TOTAL COLETADO: {len(all_deputadas)} deputadas de {current_page - 1} p√°ginas\n")
    
    # Coletar informa√ß√µes detalhadas dos perfis
    if all_deputadas:
        print("4. Coletando informa√ß√µes detalhadas dos perfis individuais...\n")
        detailed_deputadas = collect_detailed_profiles(all_deputadas, session, headers)
        return detailed_deputadas
    
    return all_deputadas


# ==========================================
# PARTE 3: EXTRA√á√ÉO DE DEPUTADAS DA P√ÅGINA
# ==========================================

def parse_deputadas_results(html_content: bytes, source_url: str) -> List[Dict]:
    """
    Analisa o conte√∫do HTML para extrair informa√ß√µes das deputadas.
    
    Estrat√©gia de extra√ß√£o:
    1. Identifica elementos HTML que cont√™m dados de deputadas
    2. Extrai informa√ß√µes b√°sicas (nome, partido, UF, etc.)
    3. Captura links para perfis individuais
    
    Args:
        html_content: Conte√∫do HTML da p√°gina de resultados
        source_url: URL de onde os dados foram extra√≠dos
        
    Returns:
        list: Lista com dados das deputadas extra√≠dos do HTML
    """
    
    soup = BeautifulSoup(html_content, 'html.parser')
    deputadas = []
    
    # Padr√µes de busca para identificar cards/elementos de deputadas
    result_patterns = [
        {'selector': '.card-deputado, .card-resultado, .deputado-resultado'},
        {'selector': 'ul.lista-deputados li, .lista-resultados li'},
        {'selector': 'table.resultados tr, .tabela-deputados tr'},
        {'selector': 'div[class*="deputado"]'},
        {'selector': 'a[href*="/deputados/"][href*="/perfil"]'}
    ]
    
    # Tentar cada padr√£o at√© encontrar elementos
    for pattern in result_patterns:
        elements = soup.select(pattern['selector'])
        
        if elements:
            # Extrair dados de cada elemento encontrado
            for element in elements:
                deputada_data = extract_deputada_from_element(element, source_url)
                
                if deputada_data and is_valid_deputada_data(deputada_data):
                    deputadas.append(deputada_data)
            
            if deputadas:
                return deputadas
    
    # Se padr√µes espec√≠ficos falharam, tentar padr√µes gerais
    general_elements = soup.select('a[href*="/deputados/"]')
    
    for element in general_elements[:50]:  # Limitar para evitar ru√≠do
        deputada_data = extract_deputada_from_element(element, source_url)
        
        if deputada_data and is_valid_deputada_data(deputada_data):
            deputadas.append(deputada_data)
    
    return deputadas


def extract_deputada_from_element(element, source_url: str) -> Optional[Dict]:
    """
    Extrai informa√ß√µes da deputada a partir de um elemento HTML.
    
    Campos extra√≠dos:
    - Nome
    - Partido
    - UF (estado)
    - Legislatura
    - Situa√ß√£o (em exerc√≠cio, licen√ßa, etc.)
    - Link do perfil
    
    Args:
        element: Elemento BeautifulSoup
        source_url: URL de origem dos dados
        
    Returns:
        dict: Dados da deputada ou None se a extra√ß√£o falhar
    """
    
    try:
        # Extrair nome usando diferentes seletores
        nome = extract_text_by_selectors(element, [
            '.nome-deputado', '.nome-resultado', '.deputado-nome',
            '.card-title', '.resultado-nome', '.nome-parlamentar',
            'h1', 'h2', 'h3', 'h4', 'h5',
            'a[href*="/deputados/"]', 'a.nome', 'a strong',
            'strong', 'b',
            'td:first-child', 'th:first-child'
        ])
        
        if not nome or len(nome) < 3:
            return None
        
        # Limpar e validar nome
        nome = clean_deputada_name(nome)
        if not nome:
            return None
        
        # Extrair partido
        partido = extract_text_by_selectors(element, [
            '.partido', '.sigla-partido', '.partido-deputado',
            '.card-partido', '.resultado-partido',
            'td:nth-child(2)', '.party', '.sigla'
        ])
        
        # Extrair estado/UF
        uf = extract_text_by_selectors(element, [
            '.uf', '.estado', '.sigla-uf', '.card-uf',
            '.resultado-uf', '.estado-deputado',
            'td:nth-child(3)', '.state'
        ])
        
        # Extrair legislatura
        legislatura = extract_text_by_selectors(element, [
            '.legislatura', '.mandato', '.periodo',
            'td:nth-child(4)'
        ])
        
        # Extrair situa√ß√£o
        situacao = extract_text_by_selectors(element, [
            '.situacao', '.status', '.exercicio',
            'td:nth-child(5)'
        ])
        
        # Extrair link do perfil
        perfil_link = ""
        link_elem = element.find('a', href=True)
        if link_elem:
            href = link_elem['href']
            if '/deputados/' in href:
                if href.startswith('http'):
                    perfil_link = href
                else:
                    perfil_link = f"https://www.camara.leg.br{href}"
        
        # Montar dicion√°rio com dados b√°sicos
        deputada_data = {
            'nome': nome,
            'partido': clean_text(partido) if partido else "",
            'uf': clean_text(uf) if uf else "",
            'legislatura': clean_text(legislatura) if legislatura else "M√∫ltiplas legislaturas",
            'situacao': clean_text(situacao) if situacao else "Conforme per√≠odo",
            'link_perfil': perfil_link,
            'fonte_dados': 'Web Scraping HTML',
            'url_fonte': source_url,
            'data_extracao': time.strftime("%Y-%m-%d %H:%M:%S"),
            'metodo_extracao': 'BeautifulSoup - C√¢mara dos Deputados (filtro sexo=F)'
        }
        
        return deputada_data
        
    except Exception as e:
        return None


# ==========================================
# PARTE 4: COLETA DE DADOS DETALHADOS
# ==========================================

def collect_detailed_profiles(deputadas: List[Dict], session: requests.Session, headers: Dict) -> List[Dict]:
    """
    Coleta informa√ß√µes detalhadas do perfil individual de cada deputada.
    
    Para cada deputada:
    1. Acessa a URL do perfil individual
    2. Extrai dados adicionais (nome civil, naturalidade, propostas, etc.)
    3. Atualiza o dicion√°rio com novos campos
    
    Args:
        deputadas: Lista com dados b√°sicos das deputadas
        session: Sess√£o HTTP
        headers: Headers HTTP
        
    Returns:
        list: Lista atualizada com dados detalhados
    """
    
    detailed_deputadas = []
    
    for i, deputada in enumerate(deputadas, 1):
        nome = deputada['nome']
        perfil_url = deputada.get('link_perfil', '')
        
        print(f"   [{i}/{len(deputadas)}] Processando: {nome}")
        
        if not perfil_url:
            print(f"               ‚úó Sem URL de perfil, pulando...")
            detailed_deputadas.append(deputada)
            continue
        
        try:
            # Fazer requisi√ß√£o para o perfil
            response = session.get(perfil_url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                # Parsear HTML do perfil
                detalhes = extract_profile_details(response.content, perfil_url)
                
                # Mesclar dados b√°sicos com detalhados
                deputada_completa = {**deputada, **detalhes}
                detailed_deputadas.append(deputada_completa)
                
                print(f"               ‚úì Dados detalhados coletados")
            else:
                print(f"               ‚úó Erro HTTP {response.status_code}")
                detailed_deputadas.append(deputada)
            
            # Delay entre requisi√ß√µes (boas pr√°ticas)
            # Aumentar delay a cada 10 perfis para n√£o sobrecarregar
            if i % 10 == 0:
                time.sleep(2)
            else:
                time.sleep(1)
            
        except Exception as e:
            print(f"               ‚úó Erro: {e}")
            detailed_deputadas.append(deputada)
    
    print()
    return detailed_deputadas


# ==========================================
# PARTE 5: EXTRA√á√ÉO DE DADOS DO PERFIL
# ==========================================

def extract_profile_details(html_content: bytes, perfil_url: str) -> Dict:
    """
    Extrai informa√ß√µes detalhadas da p√°gina de perfil individual.
    
    Usa m√∫ltiplas estrat√©gias:
    1. Busca por tags espec√≠ficas (h1, h2, etc.)
    2. Busca por padr√µes de texto com regex
    3. Busca por classes CSS comuns
    
    Campos extra√≠dos:
    - Nome civil completo
    - Partido detalhado
    - Naturalidade (cidade/estado de nascimento)
    - T√≠tulo do cargo
    - N√∫mero de propostas de autoria
    - N√∫mero de propostas relatadas
    - N√∫mero de vota√ß√µes em plen√°rio
    
    Args:
        html_content: Conte√∫do HTML do perfil
        perfil_url: URL do perfil
        
    Returns:
        dict: Dicion√°rio com dados detalhados
    """
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    detalhes = {
        'nome_civil': '',
        'partido_detalhado': '',
        'naturalidade': '',
        'titulo_cargo': '',
        'propostas_autoria': '',
        'propostas_relatadas': '',
        'votacoes_plenario': '',
        'url_perfil_detalhado': perfil_url,
        'perfil_detalhado': 'Coletado'
    }
    
    try:
        # Obter texto completo da p√°gina
        texto_completo = soup.get_text()
        
        # 1. NOME CIVIL
        # Estrat√©gia: buscar h1 ou h2 principal
        nome_elements = soup.find_all(['h1', 'h2'])
        for elem in nome_elements:
            text = elem.get_text().strip()
            if text and len(text) > 3 and len(text) < 100:
                detalhes['nome_civil'] = text
                break
        
        # 2. PARTIDO DETALHADO
        # Padr√£o: "PT - Partido dos Trabalhadores"
        partido_elements = soup.find_all(text=True)
        for elem in partido_elements:
            if elem and isinstance(elem, str):
                text = elem.strip()
                # Detectar formato "SIGLA - Nome do Partido"
                if re.match(r'^[A-Z]{2,10}\s*-\s*.+', text) and len(text) < 80:
                    detalhes['partido_detalhado'] = text[:50]
                    break
        
        # 3. NATURALIDADE
        # Padr√£o: "Natural de S√£o Paulo/SP" ou "Naturalidade: Rio de Janeiro"
        nat_match = re.search(
            r'(?:Natural de|Naturalidade)[:\s]*([A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á][^\.;\n]{3,80})',
            texto_completo,
            re.IGNORECASE
        )
        if nat_match:
            naturalidade = nat_match.group(1).strip()
            naturalidade = re.sub(r'\s+', ' ', naturalidade)
            detalhes['naturalidade'] = naturalidade[:100]
        
        # 4. T√çTULO DO CARGO
        # Padr√£o: "TITULAR" ou "EM EXERC√çCIO" ou "DEPUTADA FEDERAL"
        titulo_keywords = ['TITULAR', 'EXERC√çCIO', 'DEPUTAD']
        titulo_elements = soup.find_all(['h2', 'h3', 'p', 'div', 'span'])
        for elem in titulo_elements:
            text = elem.get_text().strip().upper()
            if any(keyword in text for keyword in titulo_keywords):
                if len(text) < 50:  # Evitar textos longos
                    detalhes['titulo_cargo'] = text
                    break
        
        # 5-7. N√öMEROS (propostas e vota√ß√µes)
        # Buscar n√∫meros na p√°gina que provavelmente representam estat√≠sticas
        all_numbers = []
        for elem in soup.find_all(['span', 'div', 'strong', 'b']):
            text = elem.get_text().strip()
            if text.isdigit() and len(text) <= 4 and int(text) > 0:
                all_numbers.append(text)
        
        # Atribuir primeiros n√∫meros encontrados como estat√≠sticas
        if len(all_numbers) > 0:
            detalhes['propostas_autoria'] = all_numbers[0]
        if len(all_numbers) > 1:
            detalhes['votacoes_plenario'] = all_numbers[1]
        if len(all_numbers) > 2:
            detalhes['propostas_relatadas'] = all_numbers[2]
    
    except Exception as e:
        # Se houver erro na extra√ß√£o, retornar campos vazios (n√£o quebrar o programa)
        pass
    
    return detalhes


# ==========================================
# PARTE 6: FUN√á√ïES AUXILIARES
# ==========================================

def extract_text_by_selectors(element, selectors: List[str]) -> str:
    """
    Extrai texto usando m√∫ltiplos seletores CSS.
    Tenta cada seletor at√© encontrar um elemento v√°lido.
    """
    for selector in selectors:
        try:
            elem = element.select_one(selector)
            if elem:
                text = elem.get_text().strip()
                if text and len(text) > 1:
                    return text
        except:
            continue
    return ""


def clean_deputada_name(name: str) -> str:
    """
    Limpa e valida nome de deputada.
    Remove textos indesejados e valida formato.
    """
    if not name:
        return ""
    
    # Remover textos indesejados (elementos de interface)
    unwanted_phrases = [
        "pesquise", "deputado", "filtro", "buscar", "resultado",
        "p√°gina", "menu", "navega√ß√£o", "ver mais", "clique"
    ]
    
    name_lower = name.lower()
    for phrase in unwanted_phrases:
        if phrase in name_lower:
            return ""
    
    # Limpar texto
    name = name.strip()
    
    # Verificar se parece um nome v√°lido
    if len(name) < 3 or len(name) > 100:
        return ""
    
    # Verificar se cont√©m pelo menos uma letra
    if not any(c.isalpha() for c in name):
        return ""
    
    return name


def clean_text(text: str) -> str:
    """Limpa texto gen√©rico."""
    if not text:
        return ""
    return text.strip()[:50]


def is_valid_deputada_data(deputada_data: Dict) -> bool:
    """
    Valida se os dados da deputada est√£o completos e s√£o v√°lidos.
    
    Args:
        deputada_data: Dados da deputada para validar
        
    Returns:
        bool: True se os dados s√£o v√°lidos
    """
    if not deputada_data:
        return False
    
    # Verificar campo obrigat√≥rio: nome
    name = deputada_data.get('nome', '')
    if not name or len(name) < 3:
        return False
    
    # N√£o aplicar filtro de nome feminino
    # Confiamos no filtro de g√™nero do site (sexo=F)
    
    return True


# ==========================================
# PARTE 7: SALVAMENTO EM CSV
# ==========================================

def save_to_csv(deputadas_data: List[Dict], filename: str = "../data/deputadas.csv") -> None:
    """
    Salva os dados das deputadas em arquivo CSV.
    
    CSV √© um formato intermedi√°rio antes da consolida√ß√£o JSON.
    Facilita visualiza√ß√£o, debugging e valida√ß√£o dos dados.
    
    Args:
        deputadas_data: Lista com dados das deputadas
        filename: Nome do arquivo CSV (caminho relativo)
    """
    
    if not deputadas_data:
        print("   ‚úó Nenhum dado para salvar\n")
        return
    
    try:
        # Criar diret√≥rio se n√£o existir
        Path(filename).parent.mkdir(parents=True, exist_ok=True)
        
        # Definir campos (colunas) do CSV - M√çNIMO 11 ATRIBUTOS (temos 19!)
        fieldnames = [
            'nome',                    # 1
            'nome_civil',              # 2
            'partido',                 # 3
            'partido_detalhado',       # 4
            'uf',                      # 5
            'naturalidade',            # 6
            'titulo_cargo',            # 7
            'legislatura',             # 8
            'situacao',                # 9
            'propostas_autoria',       # 10
            'propostas_relatadas',     # 11
            'votacoes_plenario',       # 12 (extra)
            'link_perfil',             # 13 (URL DE ORIGEM - OBRIGAT√ìRIO!)
            'url_perfil_detalhado',    # 14 (extra)
            'perfil_detalhado',        # 15 (extra)
            'fonte_dados',             # 16 (extra)
            'url_fonte',               # 17 (extra)
            'data_extracao',           # 18 (extra)
            'metodo_extracao'          # 19 (extra)
        ]
        
        print("5. Salvando dados em CSV...")
        print(f"   Arquivo: {filename}")
        print(f"   Campos: {len(fieldnames)} atributos (requisito: m√≠nimo 11) ‚úì")
        
        # Escrever CSV com encoding UTF-8
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            # Escrever cabe√ßalho
            writer.writeheader()
            
            # Escrever cada deputada
            for deputada in deputadas_data:
                # Garantir que todos os campos existem
                row = {field: deputada.get(field, '') for field in fieldnames}
                writer.writerow(row)
        
        print(f"   ‚úì Dados salvos com sucesso!")
        print(f"   ‚úì Total de deputadas: {len(deputadas_data)}")
        print(f"   ‚úì Caminho completo: {Path(filename).absolute()}\n")
        
    except Exception as e:
        print(f"   ‚úó Erro ao salvar CSV: {e}\n")


# ==========================================
# PARTE 8: ESTAT√çSTICAS DOS DADOS
# ==========================================

def generate_statistics(deputadas_data: List[Dict]) -> Dict:
    """
    Gera estat√≠sticas descritivas dos dados coletados.
    
    √ötil para:
    - Validar qualidade dos dados
    - Criar visualiza√ß√µes para apresenta√ß√£o
    - Incluir nos slides (an√°lise estat√≠stica)
    
    Args:
        deputadas_data: Lista com dados das deputadas
        
    Returns:
        dict: Estat√≠sticas calculadas
    """
    
    if not deputadas_data:
        return {}
    
    stats = {
        "total_deputadas": len(deputadas_data),
        "por_partido": {},
        "por_uf": {}
    }
    
    # Contar distribui√ß√£o por partido e UF
    for deputada in deputadas_data:
        partido = deputada.get('partido', 'N/A')
        uf = deputada.get('uf', 'N/A')
        
        stats["por_partido"][partido] = stats["por_partido"].get(partido, 0) + 1
        stats["por_uf"][uf] = stats["por_uf"].get(uf, 0) + 1
    
    # Ordenar por quantidade (decrescente)
    stats["por_partido"] = dict(sorted(stats["por_partido"].items(), key=lambda x: x[1], reverse=True))
    stats["por_uf"] = dict(sorted(stats["por_uf"].items(), key=lambda x: x[1], reverse=True))
    
    return stats


# ==========================================
# PARTE 9: FUN√á√ÉO MAIN (ORQUESTRA√á√ÉO)
# ==========================================

def main():
    """
    Fun√ß√£o principal que orquestra todo o processo de scraping.
    
    Fluxo de execu√ß√£o:
    1. Executar scraping (coletar dados b√°sicos e detalhados)
    2. Gerar estat√≠sticas descritivas
    3. Salvar dados em CSV
    4. Mostrar resumo e amostra dos dados
    """
    
    print("\n")
    print("‚ïî" + "‚ïê" * 68 + "‚ïó")
    print("‚ïë" + " " * 14 + "WEB SCRAPING - DEPUTADAS FEDERAIS" + " " * 21 + "‚ïë")
    print("‚ïö" + "‚ïê" * 68 + "‚ïù")
    print()
    
    # 1. Executar scraping
    deputadas_data = scrape_deputadas_list()
    
    if deputadas_data:
        print("=" * 70)
        print("SCRAPING CONCLU√çDO COM SUCESSO! ‚úì")
        print("=" * 70)
        
        # 2. Gerar estat√≠sticas
        print("\n6. Gerando estat√≠sticas dos dados...\n")
        stats = generate_statistics(deputadas_data)
        
        if stats:
            print("   ESTAT√çSTICAS FINAIS:")
            print("   " + "-" * 50)
            print(f"   Total de deputadas: {stats['total_deputadas']}")
            
            print(f"\n   Distribui√ß√£o por partido (Top 10):")
            for i, (partido, count) in enumerate(list(stats['por_partido'].items())[:10], 1):
                barra = "‚ñà" * min(count, 30)  # Limitar barra visual
                print(f"      {i:2}. {partido:15} {barra} {count}")
            
            print(f"\n   Distribui√ß√£o por estado (Top 10):")
            for i, (uf, count) in enumerate(list(stats['por_uf'].items())[:10], 1):
                print(f"      {i:2}. {uf}: {count}")
        
        # 3. Salvar em CSV
        print("\n" + "=" * 70)
        save_to_csv(deputadas_data)
        
        # 4. Mostrar amostra dos dados coletados
        print("=" * 70)
        print("AMOSTRA DOS DADOS (primeiras 3 deputadas):")
        print("-" * 70)
        for i, deputada in enumerate(deputadas_data[:3], 1):
            print(f"\n{i}. {deputada['nome']}")
            print(f"   Partido: {deputada['partido']} | UF: {deputada['uf']}")
            print(f"   Legislatura: {deputada['legislatura']}")
            print(f"   Situa√ß√£o: {deputada['situacao']}")
            if deputada.get('naturalidade'):
                print(f"   Naturalidade: {deputada['naturalidade']}")
            if deputada.get('propostas_autoria'):
                print(f"   Propostas: {deputada['propostas_autoria']}")
            print(f"   Perfil: {deputada['link_perfil']}")
        
        print()
        
    else:
        print("=" * 70)
        print("FALHA NO SCRAPING - Nenhum dado coletado ‚úó")
        print("=" * 70)
        print("\nPOSS√çVEIS CAUSAS:")
        print("  ‚Ä¢ Site mudou estrutura HTML")
        print("  ‚Ä¢ Bloqueio anti-bot (403 Forbidden)")
        print("  ‚Ä¢ Problemas de conectividade (timeout)")
        print("  ‚Ä¢ Filtro de g√™nero n√£o funcionou")
        print("\nSUGEST√ïES:")
        print("  ‚Ä¢ Verificar se a URL ainda est√° acess√≠vel")
        print("  ‚Ä¢ Testar manualmente no navegador")
        print("  ‚Ä¢ Verificar estrutura HTML da p√°gina")
        print()
    
    print("=" * 70)
    print("FIM DO PROCESSO")
    print("=" * 70)
    print()
    
    return deputadas_data


# ==========================================
# PONTO DE ENTRADA DO SCRIPT
# ==========================================

if __name__ == "__main__":
    # Executar fun√ß√£o main quando script for rodado diretamente
    # (n√£o quando importado como m√≥dulo)
    main()